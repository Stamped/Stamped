
Searching Approaches:
    * Separate reverse-index table; query term => entities / stamps?
    * Augment stamp with cache of entity data (e.g., title, category, subcategory, etc.)
    * Augment stamp with cache of extracted entity keywords

*** How to handle restricting search to a subset of stamps or entities?
    * break search up into buckets w/ each bucket containing at most n items


    * add viewport sanity checking in HTTPGenericSlice => GenericSlice conversion
    * test map reduce sorting extensively, including targeted unit tests
    * improve error messages for get_dns_by_node_name
    * popbar => po special case of name checking; make fuzzy name deduping based on name prefix s.t. leniancy is inversely proportional to string length (e.g., shorter strings should have harsher requirements for matching)
    * possibly also do prefix-based matching after breaking up into words
    * cleanup userRequest instances; enforce screen_name if given in addition to user_id
    * what happens if user_id or screen_name lookup fails?

API v1:
    * entrypoints:
        * collections/inbox
        * collections/user
        * favorites/show
        * todo's

class HTTPGenericSlice(Schema):
    def setSchema(self):
        # paging
        self.limit              = SchemaElement(int)
        self.offset             = SchemaElement(int)
        
        # sorting
        # (relevance, popularity, proximity, created, modified, alphabetical)
        self.sort               = SchemaElement(basestring)
        self.reverse            = SchemaElement(bool)
        self.center             = SchemaElement(basestring) # "lat,lng"
        
        # filtering
        self.query              = SchemaElement(basestring)
        self.category           = SchemaElement(basestring)
        self.subcategory        = SchemaElement(basestring)
        self.since              = SchemaElement(int)
        self.before             = SchemaElement(int)
        self.deleted            = SchemaElement(bool)
        self.viewport           = SchemaElement(basestring) # "lat0,lng0;lat1,lng1"
        
        # misc options
        self.quality            = SchemaElement(int)
        self.comments           = SchemaElement(bool)

class HTTPUserCollectionSlice(HTTPGenericSlice):
    def setSchema(self):
        HTTPGenericSlice.setSchema(self)
        
        self.user_id            = SchemaElement(basestring)
        self.screen_name        = SchemaElement(basestring)

API:
    * standardize handleHTTPRequest to take in a parameter for whether or not auth is required and input schema

blog:
    * scaling considerations
        * modular; every component should support async w/ timeouts, fallbacks & monitoring / alerts if dependency fails

platform:
    * redirect useless statsd.node.log to /dev/null
    * handle async errors gracefully - email + retryable
    * monitor process should check mongo's replica set status
    * add integrity checking daemon as weekly cron job
    
    * cleanup and optimizations of _convertSearchId
    * alerts => async tasks
    * stable ids for merged entities

web:
    * web deployment script which compiles / optimizes all javascript, ensures assets are up-to-date, etc.
    * take a look at stamped-web repository

data:
    * import top songs from hypem (http://hypem.com/popular?ax=1)
    * import top songs from wearehunted (http://wearehunted.com/a/#/emerging/)
    * import top songs / albums from Pitchfork
    * see: https://github.com/jgallen23/playlist-import-example

Bieber:
    * limit activity notifications to last N or so
    * collapse adjacent activity items into each other

* app companies being returned as 'artist'
* debug distributing reads to secondaries

* cleanup invocation on ./deploy.py, supporting suboptions and subcommands
    * possibly write separate optparse?
    * remove stale / deprecated deploy code

* continuous integration
    * how to see current revision in git?
* investigate autocomplete and add cron job
* cap tempentities
* remove iTunes affiliate search dependency
* remove linked entity corruption from activity collection

OLD:
    * stamp image async resizing
    * update stamps/create API for async image uploads
    * memcached
    * 6 API instances - 3 per AZ
    * cleanup django / gunicorn www and httpapi layout
    * add iTunes apps to entity db
    * cycle DB nodes in replica set
    * investigate entity shifting
    * investigate custom AMI to reduce deploy time / errors
    * make nginx and gunicorn daemons
    * rewrite node initialization logic
        * remove dependency on ganglia
        * instance-specific means of saying whether or not a node is up?
    * API fixes for incremental loading
        * credit
        * comments
        * already implemented: TODO, inbox, news, profile
        * don't touch: followers, friends
    * audit cron jobs
        * bootstrap/bin/update_db.py
        * update_apple -- should NOT be on api instances
    * switch opentable links with new mobile versions
    * unable to find primary? try a different node...
    * look into renaming terminal title when ./connect'ing into a machine
        * setting hostname of remote instance should do the trick
    * improve monitoring notifications for prolonged outages
    * ELB cloudwatch metrics => monitoring solution
    * move custom_entities cron job over to monitor instance and investigate why it's taking so long to run with high CPU (could just be since it was being run 9 separate times concurrently)
    * amazon product API
    * twitter/fb integration on the backend
    
    * async task queue for bottlenecks (e.g., NYMag stamping something => writing to all followers' inboxes synchronously before stamp completes)
        * local / synchronous fallback if async message broker is unavailable
    * cache amazon product API queries and look into utilizing multiple API keys (possibly one per API server)
    * add cache expires control to memcached_function
    * refactor repo layout
    * ensure async tasks are logged and accessible from logger
    * remove ghost mongod process
    * cron job only running once
    * integrity checking daemon
    * look into integrating foursquare APIs
    * refactor repository layout
    * automated way to determine whether or not we're on PROD stack
    * add wrapper to APITasks.py to invoke tasks with specific args
    * improve deploy update script to rollout updates one node at a time and be ELB-aware
    * generic, private logging endpoint for clients
    * change bootstrap/bin/update.py to return non-zero exit status if WARNINGs encountered
    
    34.026401,-118.38947899

gman () {
    man -t "${1}" | open -f -a /Applications/gvim
}

db.users.group({ reduce: function(obj,prev) { prev.count = obj.stats.num_stamps_left }, %sinitial: {count: 0, }, key:{_id:1}, finalize: function(obj) { return { "count" : obj.count }}})

db.stamps.group({ reduce: function(obj,prev) { prev.count += 1; }, initial: { count : 0 }, key:{"entity.category" : 1}, })

db.logstats.group({ reduce: function(obj,prev) { prev.users[obj.uid] = 1; }, cond:{uid: {$exists: true}, bgn: {$gte: new Date(new Date() - 1 * 60 * 60000)}}, initial: {users:{}, }, key:{}, })

db.stamps.group({ reduce: function(obj,prev) { prev.count += 1; }, initial: { count:0 }, key:{"entity.entity_id":1}})

db.users.find({}, {"_id" : 0, "screen_name" : 1, "stats.num_followers" : 1}).sort({"stats.num_followers" : -1}).limit(20)

map = function() {
  emit({day: day, user_id: this.user_id}, {count: 1});
}


db.users.group({ reduce: function(obj,prev) { if (obj.stats.hasOwnProperty("num_followers")) {prev.count += obj.stats.num_followers; } }, initial: { count:0 }, key:{}})



db.users.group({ reduce: function(obj,prev) { prev.count += 1; n = 0; if (obj.stats.hasOwnProperty("num_followers")) {n = obj.stats.num_followers; prev.sum += n; } if (n > prev.max) { prev.max = n; } if (n < prev.min) { prev.min = n}}, initial: { count : 0, sum : 0, max : -1, min : 9999 }, key:{}, finalize : function(obj) { return { "avg" : obj.sum / obj.count, "min" : obj.min, "max" : obj.max, }}})

db.users.group({ reduce: function(obj,prev) { prev.count += 1; if (obj.stats.hasOwnProperty("num_followers")) { prev.sum += obj.stats.} }, initial: { count : 0, sum : 0 }, key:{}, })


script to find false positives from google places


# URI STATS
db.logstats.group({reduce: function(obj,prev) { prev.csum += 1; },initial: {csum:0}, key:{uri:1}})

# USERS / ACTIONS PER HOUR
db.logstats.group({reduce: function(obj,prev) { prev.users[obj.uid] = 1; prev.count += 1; },cond:{uid: {$exists: true}}, initial: {users:{}, count:0}, $keyf: function(obj) { var a = {}; d = obj.bgn.toLocaleDateString(); a['date'] = d; h = obj.bgn.getHours(); a['hour'] = h; return a;}});

# QPS / UNIQUE USER COUNT PER 5 MINUTES

db.logstats.group({reduce: function(obj,prev) { prev.users[obj.uid] = 1; prev.count += 1; }, cond:{uid: {$exists: true}, bgn: {$gte: new Date(new Date() - 120 * 60000)}}, initial: {users:{}, count:0}, $keyf: function(obj) { var a = {}; bgn = new Date(obj.bgn.getFullYear(), obj.bgn.getMonth(), obj.bgn.getDate(), obj.bgn.getHours(), (Math.round(obj.bgn.getMinutes() / 5) * 5), 0, 0); a['date'] = bgn; return a;}, finalize: function(obj) {var out = {}; out['date'] = obj.date; out['qps'] = obj.count / 5.0 / 60; out['users'] = 0; for (var i in obj.users) { if (obj.users.hasOwnProperty(i)) out['users']++;} return out}});

db.logstats.group({
    reduce: function(obj,prev) { prev.users[obj.uid] = 1; prev.count += 1; }, 
    cond:{uid: {$exists: true}, bgn: {$gte: new Date(new Date() - 120 * 60000)}}, 
    initial: {users:{}, count:0}, 
    $keyf: function(obj) { var a = {}; bgn = new Date(obj.bgn.getFullYear(), obj.bgn.getMonth(), obj.bgn.getDate(), obj.bgn.getHours(), (Math.round(obj.bgn.getMinutes() / 5) * 5), 0, 0); a['date'] = bgn; return a;}, 
    finalize: function(obj) {var out = {}; out['date'] = obj.date; out['qps'] = obj.count / 5.0 / 60; out['users'] = 0; for (var i in obj.users) { if (obj.users.hasOwnProperty(i)) out['users']++;} return out}
});

db.logstats.group({ reduce: function(obj,prev) { prev.users[obj.uid] = 1; }, cond:{uid: {$exists: true}, bgn: {$gte: new Date(new Date() - 24 * 60 * 60000)}}, initial: {users:{}, }, key:{}, });

db.comments.group({ reduce: function(obj,prev) { prev.count += 1 }, cond:{stamp_id: {$exists: true} }, initial: {count: 0, }, key:{stamp_id:1}, });

rabbitmqctl list_queues name messages consumers

db.entities.find({"subtitle": "food", "details.place.address": { $exists: true }}).forEach( function (doc) { doc.subtitle = doc.details.place.address; db.entities.save(doc); });

db.stamps.find().forEach(function(doc) {doc.entity = { "entity_id" : doc.entity.entity_id }; db.stamps.save(doc);})
db.stamps.find().forEach(function(doc) {e=db.entities.find({"_id" : ObjectId(doc.entity.entity_id)}); if (e.length() != 1) {return;} e = e[0]; try{if ('title' in e) {doc.entity.title=e.title.toLowerCase();}} catch(e) {} try{if ('category' in e) {doc.entity.category=e.category;}} catch(e){} try{if ('subcategory' in e) {doc.entity.subcategory=e.subcategory;}} catch(e) {} try{if ('coordinates' in e) {doc.entity.coordinates=e.coordinates;}} catch(e) {} db.stamps.save(doc);})


StampedAPI:
    * TESTING
        * need to add comprehensive StampedAPI function-specific unit tests
            * would ideally target both httpapi and StampedAPI versions
        * 
    * CLEANUP
        * regexes being recompiled in several places: should only be compiled once
        * revisit schema import/export
        * rollback, especially in addStamp needs to be reassessed
            * once stamp is committed, everything else should be async and isolated from the core stamp transaction
        * move _convertSearchId to separate location, possibly in entity matcher
        * rename favorites todos
        * validateAccount helper => updateAccountSettings (checking account validity appears several times)
        * cleanup checkAccount exception handling
    * ASYNC
        * updating all stats (small writes, but totally async / isolated / low priority)
    * CACHE
        * 
    * _enrichStampObjects should be filled in LAZILY!
        * lots of enriched info is never used for many API calls..
        * horribly unoptimized; querying all favorites & likes for a user
        * e.g., removeLike goes through _enrichStampObjects
    * _getStampCollection
        * currently retrieving *all* stamps in the collection and then slicing API-side; must do this on DB side
    * reusable cursor pattern for generic retrieval of unbounded lists (e.g., collections, friends, comments, credit, etc.)
        * replace current slicing strategy


mongo stamped --eval "db.dropDatabase()"

# Mac OS X
ARCHFLAGS="-arch i386 -arch x86_64" pip install pylibmc==1.2.0

